{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBL Model Development: Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Data Function (switching to using radius)\n",
    "\n",
    "def read_and_process_data(directory_path):\n",
    "    data_frames = []\n",
    "    columns_to_extract = ['radius_X', 'radius_Y', 'radius_Z', 'radius_Ox', 'radius_Oy', 'radius_Oz', 'Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "    # Assuming each cycle has exactly 356 data points\n",
    "    total_data_points = 356\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            participant = int(os.path.basename(file_path).split('_')[0])\n",
    "            cycle_id = os.path.basename(file_path).split('_')[1].split('.')[0]  # Extract cycle_id\n",
    "            intensity = cycle_id[:4]  # Extract the first four characters of cycle_id as intensity\n",
    "\n",
    "            # Read data from CSV and select only the desired columns\n",
    "            df = pd.read_csv(file_path, usecols=columns_to_extract)\n",
    "\n",
    "            # Add participant ID, cycle_id, and participant_cycle_id as features\n",
    "            df['Participant'] = participant\n",
    "            df['Cycle_ID'] = cycle_id\n",
    "            df['Participant_Cycle_ID'] = f\"{participant}_{cycle_id}\"\n",
    "            #df['Intensity'] = intensity\n",
    "\n",
    "            # Since Data will get shuffled (idk why but shuffling makes the model so much better.. ??)\n",
    "            # So thus, need to store original index values \n",
    "            df['Original_Index'] = df.index\n",
    "\n",
    "            # # Add normalized_cycle_position\n",
    "            # df['Normalized_Cycle_Position'] = df.index / (total_data_points - 1)\n",
    "\n",
    "            if (intensity == \"HIIT\"):\n",
    "                df['Intensity'] = .90\n",
    "            else:\n",
    "                df['Intensity'] = .50\n",
    "            \n",
    "            # df['Intensity'] = intensity  # this is either \"HIIT\" or \"MICT\"\n",
    "\n",
    "            data_frames.append(df)\n",
    "\n",
    "    # Concatenate all data frames\n",
    "    processed_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Merge with participant weights\n",
    "    weights_df = pd.read_csv(\"Participant Weights.csv\")\n",
    "    weights_df['Weight'] = weights_df['Weight'].astype(float)\n",
    "    weights_df['Wingspan'] = weights_df['Wingspan'].astype(float)\n",
    "    processed_data = pd.merge(processed_data, weights_df, left_on='Participant', right_on='Participant')\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  8]\n",
      "for the train\n",
      "[ 1  2  3  4  5  6  9 10 11 12 13 14 16 17 18 19 20 21]\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "data_directory = \"Processed Data for ML\"\n",
    "output_folder = \"Train and Test Data (NEW 6)\" # no shuffling for trial 3\n",
    "\n",
    "# Read and process data\n",
    "data = read_and_process_data(data_directory)\n",
    "\n",
    "# Get unique participants\n",
    "participants = data['Participant'].unique()\n",
    "\n",
    "# Changing the order of the participants (now we're sorting it from 1-21)\n",
    "participants = np.sort(participants)\n",
    "\n",
    "# Check that we have exactly 20 participants\n",
    "assert len(participants) == 20, \"The number of participants is not equal to 20.\"\n",
    "\n",
    "# Randomly choose 2 participants to leave out for testing or manually select\n",
    "test_participants = np.random.choice(participants, 2, replace=False)\n",
    "# test_participants = [1, 13]\n",
    "# Select the participants for training\n",
    "train_participants = np.setdiff1d(participants, test_participants)\n",
    "\n",
    "print(test_participants)\n",
    "print(\"for the train\")\n",
    "print(train_participants)\n",
    "\n",
    "# Split the data into train and test based on the selected participants\n",
    "train_data = data[data['Participant'].isin(train_participants)]\n",
    "test_data = data[data['Participant'].isin(test_participants)]\n",
    "\n",
    "train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Specify the output columns\n",
    "output_columns = ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']\n",
    "\n",
    "# Create X (input) and y (output) for train and test\n",
    "X_train = train_data.drop(output_columns, axis=1)\n",
    "y_train = train_data[output_columns]\n",
    "X_test = test_data.drop(output_columns, axis=1)\n",
    "y_test = test_data[output_columns]\n",
    "\n",
    "data_folder = os.path.join(output_folder, \"Train Test Split\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Save train and test data to CSV\n",
    "train_data.to_csv(os.path.join(data_folder, \"train_data.csv\"), index=False)\n",
    "test_data.to_csv(os.path.join(data_folder, \"test_data.csv\"), index=False)\n",
    "\n",
    "# Save participant numbers to text files\n",
    "np.savetxt(os.path.join(data_folder, \"train_participants.txt\"), train_participants, fmt='%d')\n",
    "np.savetxt(os.path.join(data_folder, \"test_participant.txt\"), test_participants, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc  # Import the garbage collector module\n",
    "\n",
    "# Define numerical features\n",
    "numeric_features = ['radius_X', 'radius_Y', 'radius_Z', 'radius_Ox', 'radius_Oy', 'radius_Oz', 'Weight', 'Wingspan', 'Intensity']\n",
    "#categorical_features = ['Intensity']\n",
    "\n",
    "# Create transformers for numerical and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a preprocessor that applies transformers to specific columns \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        #('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=700, n_jobs=-1)\n",
    "\n",
    "# Create a pipeline with the preprocessor and the regressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_model = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mdpe_by_participant_cycle_ids(y_pred, y_test, X_test):\n",
    "    \"\"\"\n",
    "    Calculate Median Percentage Error (MDPE) for each unique participant_cycle_id for a specific output column.\n",
    "    \n",
    "    Args:\n",
    "    - y_pred (numpy array): Predicted values.\n",
    "    - y_test (numpy array): True values.\n",
    "    - X_test (DataFrame): DataFrame containing the test data including 'Participant_Cycle_ID'.\n",
    "    - output_column (str): Name of the output column.\n",
    "    \n",
    "    Returns:\n",
    "    - mdpe_scores (dict): Dictionary containing MDPE scores for each unique participant_cycle_id.\n",
    "    \"\"\"\n",
    "    mdpe_scores = []\n",
    "    \n",
    "    # Get unique participant_cycle_ids\n",
    "    unique_participant_cycle_ids = X_test['Participant_Cycle_ID'].unique()\n",
    "    \n",
    "    # Calculate MDPE for each unique participant_cycle_id\n",
    "    for unique_id in unique_participant_cycle_ids:\n",
    "        mask = X_test['Participant_Cycle_ID'] == unique_id\n",
    "        y_pred_id = y_pred[mask]\n",
    "        y_test_id = y_test[mask]\n",
    "        \n",
    "        # Exclude NaN values\n",
    "        mask_valid = ~np.isnan(y_test_id)\n",
    "        y_pred_id = y_pred_id[mask_valid]\n",
    "        y_test_id = y_test_id[mask_valid]\n",
    "        \n",
    "        mdpe = np.median((y_pred_id - y_test_id) / y_test_id * 100)\n",
    "        mdpe_scores.append(mdpe)\n",
    "        \n",
    "    return mdpe_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Column: Fx\n",
      "Average of MdPEs: -23.129211019672848\n",
      "Standard Deviation of MdPEs: 25.508001382611635\n",
      "Output Column: Fy\n",
      "Average of MdPEs: -15.949662936969176\n",
      "Standard Deviation of MdPEs: 26.99036200872597\n",
      "Output Column: Fz\n",
      "Average of MdPEs: -54.81425165380324\n",
      "Standard Deviation of MdPEs: 45.96581003868795\n",
      "Output Column: Tx\n",
      "Average of MdPEs: -4.617479602487032\n",
      "Standard Deviation of MdPEs: 30.79975880710562\n",
      "Output Column: Ty\n",
      "Average of MdPEs: -14.624375991782054\n",
      "Standard Deviation of MdPEs: 30.392853433109703\n",
      "Output Column: Tz\n",
      "Average of MdPEs: -14.443587746510715\n",
      "Standard Deviation of MdPEs: 29.803173300465087\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_directory = \"RF Outputs NEW 6\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Predict y_pred for all output columns in this fold\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=output_columns)\n",
    "\n",
    "# Save selected columns from X_test, y_test, and y_pred for this fold as CSV\n",
    "fold_output_directory = os.path.join(output_directory, \"output\")\n",
    "os.makedirs(fold_output_directory, exist_ok=True)\n",
    "\n",
    "# Save train and test data to CSV\n",
    "X_test.to_csv(os.path.join(fold_output_directory, \"X_test.csv\"), index=False)\n",
    "y_test.to_csv(os.path.join(fold_output_directory, \"y_test.csv\"), index=False)\n",
    "y_pred_df.to_csv(os.path.join(fold_output_directory, \"y_pred.csv\"), index=False)\n",
    "\n",
    "# Calculate MDPE scores for this fold and each output column\n",
    "mdpe_scores_list = []\n",
    "for i, output_col in enumerate(output_columns):\n",
    "    y_pred_fold_output = y_pred[:, i]\n",
    "    \n",
    "    mdpe_scores = calculate_mdpe_by_participant_cycle_ids(y_pred_fold_output, y_test[output_col], X_test)\n",
    "    mdpe_scores_list.append(pd.DataFrame({'Output': [output_col]*len(mdpe_scores), 'MDPE': mdpe_scores}))\n",
    "    \n",
    "    average = np.mean(mdpe_scores)\n",
    "    std = np.std(mdpe_scores)\n",
    "    print(f\"Output Column: {output_col}\")\n",
    "    print(\"Average of MdPEs:\", average)\n",
    "    print(\"Standard Deviation of MdPEs:\", std)\n",
    "    \n",
    "    # Save average and standard deviation of MdPEs to the output directory\n",
    "    with open(os.path.join(output_directory, \"mdpe_scores.txt\"), 'a') as f:\n",
    "        f.write(f\"Output Column: {output_col}\\n\")\n",
    "        f.write(f\"Average of MdPEs: {average}\\n\")\n",
    "        f.write(f\"Standard Deviation of MdPEs: {std}\\n\\n\")\n",
    "\n",
    "# Combine MDPE scores for all output columns in this fold\n",
    "mdpe_df = pd.concat(mdpe_scores_list)\n",
    "\n",
    "# Plot MDPE scores for this fold and all output columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Output', y='MDPE', data=mdpe_df, palette='husl')  # Using 'husl' palette for more colorful plots\n",
    "plt.title(f'Box-and-Whisker Plot of MDPE Scores')\n",
    "plt.ylabel('MDPE')\n",
    "plt.xlabel('Output Column')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(os.path.join(fold_output_directory, f\"MDPE_plot.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature importances:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature radius_Y: 0.4117619593867028\n",
      "Feature radius_Z: 0.1701532375007475\n",
      "Feature radius_X: 0.11841119300457034\n",
      "Feature Wingspan: 0.06829310969225834\n",
      "Feature radius_Oz: 0.06268375719089815\n",
      "Feature radius_Oy: 0.05887355198231738\n",
      "Feature Weight: 0.05463558794285864\n",
      "Feature Intensity: 0.03437848275729298\n",
      "Feature radius_Ox: 0.020809120542354013\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeature importances:\")\n",
    "\n",
    "# Extract preprocessor and regressor from the pipeline\n",
    "preprocessor = best_model.named_steps['preprocessor']\n",
    "regressor = best_model.named_steps['regressor']\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = preprocessor.transformers_[0][2]  # Numeric feature names\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = regressor.feature_importances_\n",
    "\n",
    "# Sort indices by importance\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Print feature importances\n",
    "for idx in sorted_indices:\n",
    "    print(f\"Feature {feature_names[idx]}: {feature_importances[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS307",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
